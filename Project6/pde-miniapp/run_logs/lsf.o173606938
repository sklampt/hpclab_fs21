Sender: LSF System <lsfadmin@eu-a6-009-21>
Subject: Job 173606938: <mpirun ./main 128 100 0.005 verbose> in cluster <euler> Exited

Job <mpirun ./main 128 100 0.005 verbose> was submitted from host <eu-login-11> by user <sklampt> in cluster <euler> at Thu May 27 22:18:47 2021
Job was executed on host(s) <21*eu-a6-009-21>, in queue <hpc.4h>, as user <sklampt> in cluster <euler> at Thu May 27 22:37:51 2021
</cluster/home/sklampt> was used as the home directory.
</cluster/home/sklampt/hpclab/hpclab_fs2021/Project6/pde-miniapp> was used as the working directory.
Started at Thu May 27 22:37:51 2021
Terminated at Thu May 27 22:37:53 2021
Results reported at Thu May 27 22:37:53 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun ./main 128 100 0.005 verbose
------------------------------------------------------------

Exited with exit code 15.

Resource usage summary:

    CPU time :                                   0.34 sec.
    Max Memory :                                 252 MB
    Average Memory :                             -
    Total Requested Memory :                     21504.00 MB
    Delta Memory :                               21252.00 MB
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   26 sec.
    Turnaround time :                            1146 sec.

The output (if any) follows:

rank 0/21 : (1,1) neigh N:S 3:-2 neigh E:W 1:-2 local dims 42 x 18
rank 1/21 : (2,1) neigh N:S 4:-2 neigh E:W 2:0 local dims 42 x 18
rank 2/21 : (3,1) neigh N:S 5:-2 neigh E:W -2:1 local dims 44 x 18
rank 3/21 : (1,2) neigh N:S 6:0 neigh E:W 4:-2 local dims 42 x 18
rank 4/21 : (2,2) neigh N:S 7:1 neigh E:W 5:3 local dims 42 x 18
rank 5/21 : (3,2) neigh N:S 8:2 neigh E:W -2:4 local dims 44 x 18
rank 6/21 : (1,3) neigh N:S 9:3 neigh E:W 7:-2 local dims 42 x 18
rank 7/21 : (2,3) neigh N:S 10:4 neigh E:W 8:6 local dims 42 x 18
rank 8/21 : (3,3) neigh N:S 11:5 neigh E:W -2:7 local dims 44 x 18
rank 9/21 : (1,4) neigh N:S 12:6 neigh E:W 10:-2 local dims 42 x 18
rank 10/21 : (2,4) neigh N:S 13:7 neigh E:W 11:9 local dims 42 x 18
rank 11/21 : (3,4) neigh N:S 14:8 neigh E:W -2:10 local dims 44 x 18
rank 12/21 : (1,5) neigh N:S 15:9 neigh E:W 13:-2 local dims 42 x 18
rank 13/21 : (2,5) neigh N:S 16:10 neigh E:W 14:12 local dims 42 x 18
rank 14/21 : (3,5) neigh N:S 17:11 neigh E:W -2:13 local dims 44 x 18
rank 15/21 : (1,6) neigh N:S 18:12 neigh E:W 16:-2 local dims 42 x 18
rank 16/21 : (2,6) neigh N:S 19:13 neigh E:W 17:15 local dims 42 x 18
rank 17/21 : (3,6) neigh N:S 20:14 neigh E:W -2:16 local dims 44 x 18
rank 18/21 : (1,7) neigh N:S -2:15 neigh E:W 19:-2 local dims 42 x 20
rank 19/21 : (2,7) neigh N:S -2:16 neigh E:W 20:18 local dims 42 x 20
rank 20/21 : (3,7) neigh N:S -2:17 neigh E:W -2:19 local dims 44 x 20
========================================================================
                      Welcome to mini-stencil!
version   :: MPI : 21 MPI ranks
mesh      :: 128 * 128 dx = 0.00787402
time      :: 100 time steps from 0 .. 0.005
iteration :: CG 200, Newton 50, tolerance 1e-06
========================================================================
[eu-a6-009-21:18370] *** Process received signal ***
[eu-a6-009-21:18370] Signal: Segmentation fault (11)
[eu-a6-009-21:18370] Signal code: Address not mapped (1)
[eu-a6-009-21:18370] Failing at address: 0x10
[eu-a6-009-21:18371] *** Process received signal ***
[eu-a6-009-21:18371] Signal: Segmentation fault (11)
[eu-a6-009-21:18371] Signal code: Address not mapped (1)
[eu-a6-009-21:18371] Failing at address: 0x10
[eu-a6-009-21:18381] *** Process received signal ***
[eu-a6-009-21:18381] Signal: Segmentation fault (11)
[eu-a6-009-21:18381] Signal code: Address not mapped (1)
[eu-a6-009-21:18381] Failing at address: 0x10
[eu-a6-009-21:18368] *** An error occurred in MPI_Waitall
[eu-a6-009-21:18368] *** reported by process [840368129,1]
[eu-a6-009-21:18368] *** on communicator MPI_COMMUNICATOR 3
[eu-a6-009-21:18368] *** MPI_ERR_TRUNCATE: message truncated
[eu-a6-009-21:18368] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[eu-a6-009-21:18368] ***    and potentially your MPI job)
[eu-a6-009-21:18360] 4 more processes have sent help message help-mpi-errors.txt / mpi_errors_are_fatal
[eu-a6-009-21:18360] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
