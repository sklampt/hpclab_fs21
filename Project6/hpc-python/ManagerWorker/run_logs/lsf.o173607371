Sender: LSF System <lsfadmin@eu-a2p-071>
Subject: Job 173607371: <mpirun python3 manager_worker.py 4001 4001 100> in cluster <euler> Exited

Job <mpirun python3 manager_worker.py 4001 4001 100> was submitted from host <eu-login-18> by user <sklampt> in cluster <euler> at Thu May 27 22:26:06 2021
Job was executed on host(s) <4*eu-a2p-071>, in queue <normal.4h>, as user <sklampt> in cluster <euler> at Thu May 27 22:26:21 2021
</cluster/home/sklampt> was used as the home directory.
</cluster/home/sklampt/hpclab/hpclab_fs2021/Project6/hpc-python/ManagerWorker> was used as the working directory.
Started at Thu May 27 22:26:21 2021
Terminated at Thu May 27 22:26:32 2021
Results reported at Thu May 27 22:26:32 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun python3 manager_worker.py 4001 4001 100
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   29.98 sec.
    Max Memory :                                 534 MB
    Average Memory :                             190.00 MB
    Total Requested Memory :                     4096.00 MB
    Delta Memory :                               3562.00 MB
    Max Swap :                                   -
    Max Processes :                              7
    Max Threads :                                8
    Run time :                                   36 sec.
    Turnaround time :                            26 sec.

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            eu-a2p-071
  Device name:           mlx5_2
  Device vendor ID:      0x02c9
  Device vendor part ID: 4119

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There are more than one active ports on host 'eu-a2p-071', but the
default subnet GID prefix was detected on more than one of these
ports.  If these ports are connected to different physical IB
networks, this configuration will fail in Open MPI.  This version of
Open MPI requires that every physically separate IB subnet that is
used between connected MPI processes must have different subnet ID
values.

Please see this FAQ entry for more details:

  http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_default_gid_prefix to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   eu-a2p-071
  Local device: mlx5_0
--------------------------------------------------------------------------
MPI initialized with     4 processes
[eu-a2p-071:100074] 11 more processes have sent help message help-mpi-btl-openib.txt / no device params found
[eu-a2p-071:100074] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[eu-a2p-071:100074] 3 more processes have sent help message help-mpi-btl-openib.txt / default subnet prefix
[eu-a2p-071:100074] 3 more processes have sent help message help-mpi-btl-openib.txt / error in device init
 Worker     1 has done         35 tasks
Traceback (most recent call last):
  File "manager_worker.py", line 183, in <module>
    MPI.MPI_Finalize()
AttributeError: module 'mpi4py.MPI' has no attribute 'MPI_Finalize'
 Worker     2 has done         33 tasks
 Worker     3 has done         32 tasks
Traceback (most recent call last):
  File "manager_worker.py", line 183, in <module>
    MPI.MPI_Finalize()
AttributeError: module 'mpi4py.MPI' has no attribute 'MPI_Finalize'
Traceback (most recent call last):
  File "manager_worker.py", line 183, in <module>
    MPI.MPI_Finalize()
AttributeError: module 'mpi4py.MPI' has no attribute 'MPI_Finalize'
Run took 7.564981 seconds
Traceback (most recent call last):
  File "manager_worker.py", line 183, in <module>
    MPI.MPI_Finalize()
AttributeError: module 'mpi4py.MPI' has no attribute 'MPI_Finalize'
--------------------------------------------------------------------------
mpirun noticed that the job aborted, but has no info as to the process
that caused that situation.
--------------------------------------------------------------------------
